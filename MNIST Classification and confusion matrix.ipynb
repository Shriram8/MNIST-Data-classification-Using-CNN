{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting optimizers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimizers.py\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "## class Optimizers()\n",
    "######################################################################\n",
    "\n",
    "class Optimizers():\n",
    "\n",
    "    def __init__(self, all_weights):\n",
    "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
    "        \n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        # The following initializations are only used by adam.\n",
    "        # Only initializing m, v, beta1t and beta2t here allows multiple calls to adam to handle training\n",
    "        # with multiple subsets (batches) of training data.\n",
    "        self.mt = np.zeros_like(all_weights)\n",
    "        self.vt = np.zeros_like(all_weights)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.beta1t = 1\n",
    "        self.beta2t = 1\n",
    "\n",
    "        \n",
    "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= learning_rate * grad\n",
    "\n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
    "        epsilon = 1e-8\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            #print(\"aaaaaaa\")\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            grad = gradient_f(*fargs)\n",
    "\n",
    "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
    "            self.beta1t *= self.beta1\n",
    "            self.beta2t *= self.beta2\n",
    "\n",
    "            m_hat = self.mt / (1 - self.beta1t)\n",
    "            v_hat = self.vt / (1 - self.beta2t)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ion()\n",
    "\n",
    "    def parabola(wmin):\n",
    "        return ((w - wmin) ** 2)[0]\n",
    "\n",
    "    def parabola_gradient(wmin):\n",
    "        return 2 * (w - wmin)\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "\n",
    "    wmin = 5\n",
    "    optimizer.sgd(parabola, parabola_gradient, [wmin],\n",
    "                  n_epochs=500, learning_rate=0.1)\n",
    "\n",
    "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "    optimizer.adam(parabola, parabola_gradient, [wmin],\n",
    "                   n_epochs=500, learning_rate=0.1)\n",
    "    \n",
    "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimizers\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class NeuralNetwork()\n",
    "######################################################################\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        self.classes = np.arange(10)\n",
    "        (Ttrain == classes).shape\n",
    "\n",
    "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
    "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
    "            self.n_hiddens_per_layer = []\n",
    "        else:\n",
    "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
    "\n",
    "        # Initialize weights, by first building list of all weight matrix shapes.\n",
    "        n_in = n_inputs\n",
    "        shapes = []\n",
    "        for nh in self.n_hiddens_per_layer:\n",
    "            shapes.append((n_in + 1, nh))\n",
    "            n_in = nh\n",
    "        shapes.append((n_in + 1, n_outputs))\n",
    "\n",
    "        # self.all_weights:  vector of all weights\n",
    "        # self.Ws: list of weight matrices by layer\n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Define arrays to hold gradient values.\n",
    "        # One array for each W array with same shape.\n",
    "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
    "\n",
    "        self.trained = False\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "        self.Tmeans = None\n",
    "        self.Tstds = None\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        # vector of all weights built by horizontally stacking flatenned matrices\n",
    "        # for each layer initialized with uniformly-distributed values.\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
    "        # into correct shape for each layer.\n",
    "        views = []\n",
    "        start = 0\n",
    "        for shape in shapes:\n",
    "            size =shape[0] * shape[1]\n",
    "            views.append(all_weights[start:start + size].reshape(shape))\n",
    "            start += size\n",
    "        return all_weights, views\n",
    "\n",
    "\n",
    "    # Return string that shows how the constructor was called\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
    "\n",
    "\n",
    "    # Return string that is more informative to the user about the state of this neural network.\n",
    "    def __str__(self):\n",
    "        result = self.__repr__()\n",
    "        if len(self.error_trace) > 0:\n",
    "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
    "\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate, method='adam', verbose=True):\n",
    "        '''\n",
    "train: \n",
    "  X: n_samples x n_inputs matrix of input samples, one per row\n",
    "  T: n_samples x n_outputs matrix of target output values, one sample per row\n",
    "  n_epochs: number of passes to take through all samples updating weights each pass\n",
    "  learning_rate: factor controlling the step size of each update\n",
    "  method: is either 'sgd' or 'adam'\n",
    "        '''\n",
    "\n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tmeans = T.mean(axis=0)\n",
    "            self.Tstds = T.std(axis=0)\n",
    "            \n",
    "        # Standardize X and T\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        T = (T - self.Tmeans) / self.Tstds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        if len(self.Tstds) == 1:\n",
    "            #error_convert_f = lambda err: np.sqrt(err)#[0]\n",
    "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0]# to scalar\n",
    "            #print(\"hi\")\n",
    "        else:\n",
    "            #print(\"hi2\")\n",
    "            error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "     \n",
    "        \n",
    "    def relu(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s\n",
    "\n",
    "    def grad_relu(self, s):\n",
    "        return (s > 0).astype(int)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \n",
    "        #print(\"bv\")\n",
    "        '''X assumed already standardized. Output returned as standardized.'''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        Y1=np.array(self.Ys[-1])\n",
    "        #print(Y1.shape)\n",
    "        #1/0\n",
    "        return self.Ys\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        Ys=np.array(Ys[-1])\n",
    "        #print(Ys.shape)\n",
    "        mean_sq_error = np.mean((T - Ys) ** 2)\n",
    "        return mean_sq_error\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.Ys[-1]\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        #print(\"hi\")\n",
    "        #print(delta.shape)\n",
    "        \n",
    "        \n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        #print(n_layers)\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            #print(\"hi2\")\n",
    "            #print(self.dE_dWs[layeri][1:, :].shape)\n",
    "            #print(self.Ys[layeri].shape)\n",
    "            #1/0\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "                #print(\"ff\")\n",
    "        return self.all_gradients\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optimizers\n",
    "import sys\n",
    "import numpy as np\n",
    "class NeuralNetworkClassifier(NeuralNetwork):\n",
    "  \n",
    "    \n",
    "    def train(self, X, T, n_epochs, learning_rate, method='adam', verbose=True):\n",
    "        \n",
    "        if self.Xmeans is None:\n",
    "            \n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tstds=T.std(axis=0)\n",
    "            \n",
    "        \n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        \n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "       \n",
    "        error_convert_f = lambda err: np.sqrt(err)#[0] # to scalar\n",
    "            #print(\"ko\")\n",
    "            #error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        #else:\n",
    "            #error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
    "        \n",
    "        \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        \n",
    "        \n",
    "        #print('a')\n",
    "        \n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        \n",
    "        return self.Ys\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "            \n",
    "    def softmax(self,X):\n",
    "        \n",
    "        fs = np.exp(X)  # N x K\n",
    "        denom = np.sum(fs, axis=1).reshape((-1, 1))\n",
    "        gs = fs / denom\n",
    "        \n",
    "        return gs\n",
    "\n",
    "    def makeIndicatorVars(self,T):\n",
    "        # Make sure T is two-dimensional. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1)) \n",
    "        #print(\"red\") \n",
    "        return (T == np.unique(T)).astype(int)\n",
    "\n",
    "    #TtrainI = makeIndicatorVars(T)\n",
    "      \n",
    "          #neg_log_likelihood\n",
    "    def error_f(self, X, T):\n",
    "        \n",
    "        Ys = self.forward_pass(X)\n",
    "        Ys=np.array(Ys[-1])\n",
    "        #print(T.shape)\n",
    "        #print(\"help\")\n",
    "        #print(Ys.shape)\n",
    "        #1/0\n",
    "        # w = warg.reshape((-1,K))  \n",
    "        Y = self.softmax(Ys)  \n",
    "        #print(\"help\")\n",
    "        #1/0\n",
    "        #print(- np.mean(self.makeIndicatorVars(T) * np.log(Y)))\n",
    "        return - np.mean(self.makeIndicatorVars(T) * np.log(Y))\n",
    "    \n",
    "    def gradient_neg_log_likelihood(self,Ys,T):\n",
    "        \n",
    "        #print(\"kil\")\n",
    "        #print(self.Ys[-1].shape )\n",
    "    \n",
    "        Y = self.softmax(self.Ys[-1])\n",
    "        #print(\"Y\")\n",
    "        #print(Y.shape)\n",
    "        #1/0\n",
    "        #print((self.makeIndicatorVars(T).shape))\n",
    "        #print(\"one\")\n",
    "        #1/0\n",
    "        #* self.makeIndicatorVars(T).shape[1]))\n",
    "        #print(Ys.T.shape)\n",
    "        #1/0\n",
    "        #self.Ys[-1].T @\n",
    "        #print(Ttrain.shape)\n",
    "        #print(\"redbull1\")\n",
    "        #print(T.shape)\n",
    "        #1/0\n",
    "        grad =  (Y - self.makeIndicatorVars(T)) / (self.makeIndicatorVars(T).shape[0] * self.makeIndicatorVars(T).shape[1])\n",
    "        #print(\"redbull2\")\n",
    "        #print(grad.shape)\n",
    "        #print(\"one\")\n",
    "        #1/0\n",
    "        return grad\n",
    "        \n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = self.makeIndicatorVars(T) - self.Ys[-1]\n",
    "        #print(\"red\")\n",
    "        #print(T.shape)\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = self.makeIndicatorVars(T).shape[1]\n",
    "        #print(\"bull\")\n",
    "        #1/0\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        delta1=self.gradient_neg_log_likelihood(self.Ys[-1],T)\n",
    "        #print(\"bekow\")\n",
    "        #print(delta1.shape)\n",
    "        #1/0\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            #print(\"hi\")\n",
    "            #1/0\n",
    "            # gradient of all but bias weights\n",
    "            #print((self.Ys[layeri]).shape)\n",
    "            #print(self.dE_dWs[layeri][1:, :].shape)\n",
    "            #print(\"done\")\n",
    "            #print(delta.shape)\n",
    "\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta1\n",
    "            #print(\"hi2\")\n",
    "            \n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta1, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta1 = delta1 @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta1 = delta1 @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "            #print(\"redbull\")\n",
    "            #1/0\n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self,X):\n",
    "        \n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X)\n",
    "        Y = Ys[-1]\n",
    "        prob=self.softmax(Y)\n",
    "        self.classes =np.argmax(prob[:,:],axis=1)\n",
    "        \n",
    "        return self.classes,prob\n",
    "\n",
    "        \n",
    "    \n",
    "            \n",
    "                    \n",
    "            \n",
    "        #return self\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000, 1) (10000, 784) (10000, 1) (10000, 784) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "\n",
    "Xtrain = train_set[0]\n",
    "Ttrain = train_set[1].reshape(-1, 1)\n",
    "\n",
    "Xval = valid_set[0]\n",
    "Tval = valid_set[1].reshape(-1, 1)\n",
    "\n",
    "Xtest = test_set[0]\n",
    "Ttest = test_set[1].reshape(-1, 1)\n",
    "\n",
    "print(Xtrain.shape, Ttrain.shape,  Xval.shape, Tval.shape,  Xtest.shape, Ttest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.arange(10)\n",
    "(Ttrain == classes).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "learning_rate = 0.01\n",
    "\n",
    "np.random.seed(142)\n",
    "\n",
    "nnet = NeuralNetworkClassifier(Xtrain.shape[1], [5], len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 1 Error=0.48043\n",
      "Adam: Epoch 2 Error=0.46889\n",
      "Adam: Epoch 3 Error=0.46022\n",
      "Adam: Epoch 4 Error=0.45391\n",
      "Adam: Epoch 5 Error=0.44925\n"
     ]
    }
   ],
   "source": [
    "nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkClassifier(784, [5], 10, 'tanh') trained for 5 epochs, final training error 0.4492\n"
     ]
    }
   ],
   "source": [
    "print(nnet)  # uses the __str__ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "Y_classes, Y_Probs = nnet.use(Xtest)\n",
    "print(Y_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Train</td>\n",
       "      <td>0.09864</td>\n",
       "      <td>0.11356</td>\n",
       "      <td>0.09936</td>\n",
       "      <td>0.10202</td>\n",
       "      <td>0.09718</td>\n",
       "      <td>0.09012</td>\n",
       "      <td>0.09902</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.09684</td>\n",
       "      <td>0.09976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Tval</td>\n",
       "      <td>0.09910</td>\n",
       "      <td>0.10640</td>\n",
       "      <td>0.09900</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.09830</td>\n",
       "      <td>0.09150</td>\n",
       "      <td>0.09670</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.10090</td>\n",
       "      <td>0.09610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ttest</td>\n",
       "      <td>0.09800</td>\n",
       "      <td>0.11350</td>\n",
       "      <td>0.10320</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.09820</td>\n",
       "      <td>0.08920</td>\n",
       "      <td>0.09580</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>0.09740</td>\n",
       "      <td>0.10090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7  \\\n",
       "0  Train  0.09864  0.11356  0.09936  0.10202  0.09718  0.09012  0.09902   \n",
       "1   Tval  0.09910  0.10640  0.09900  0.10300  0.09830  0.09150  0.09670   \n",
       "2  Ttest  0.09800  0.11350  0.10320  0.10100  0.09820  0.08920  0.09580   \n",
       "\n",
       "        8        9       10  \n",
       "0  0.1035  0.09684  0.09976  \n",
       "1  0.1090  0.10090  0.09610  \n",
       "2  0.1028  0.09740  0.10090  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "result = []\n",
    "result.append(['Train', *(Ttrain == classes).sum(axis=0) / Ttrain.shape[0]])\n",
    "result.append(['Tval', *(Tval == classes).sum(axis=0) / Tval.shape[0]])\n",
    "result.append(['Ttest', *(Ttest == classes).sum(axis=0) / Ttest.shape[0]])\n",
    "pandas.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_classes, Ttest):\n",
    "    table = []\n",
    "    for true_class in range(10):\n",
    "        row = []\n",
    "        Ttest=Ttest.reshape(-1)\n",
    "        for predicted_class in range(10):\n",
    "            row.append(100 * np.mean(Y_classes[Ttest == true_class] == predicted_class))\n",
    "        table.append(row)\n",
    "    table\n",
    "    \n",
    "    def T_unique(T):\n",
    "        return np.unique(T)\n",
    "    Ttest_names=T_unique(Ttest)\n",
    "    Y_classes=T_unique(Y_classes)\n",
    "    return pandas.DataFrame(table, index=Ttest_names, columns=Y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>43.265306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.122449</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>1.020408</td>\n",
       "      <td>13.061224</td>\n",
       "      <td>41.122449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.559471</td>\n",
       "      <td>41.850220</td>\n",
       "      <td>18.854626</td>\n",
       "      <td>15.594714</td>\n",
       "      <td>0.088106</td>\n",
       "      <td>0.440529</td>\n",
       "      <td>3.524229</td>\n",
       "      <td>0.088106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.034884</td>\n",
       "      <td>0.096899</td>\n",
       "      <td>59.496124</td>\n",
       "      <td>33.720930</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.484496</td>\n",
       "      <td>0.678295</td>\n",
       "      <td>2.519380</td>\n",
       "      <td>0.096899</td>\n",
       "      <td>0.096899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>10.693069</td>\n",
       "      <td>80.594059</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>2.673267</td>\n",
       "      <td>4.158416</td>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.396040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>27.800407</td>\n",
       "      <td>4.073320</td>\n",
       "      <td>11.405295</td>\n",
       "      <td>0.814664</td>\n",
       "      <td>27.393075</td>\n",
       "      <td>3.564155</td>\n",
       "      <td>0.814664</td>\n",
       "      <td>4.276986</td>\n",
       "      <td>0.509165</td>\n",
       "      <td>19.348269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>10.089686</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>7.399103</td>\n",
       "      <td>11.995516</td>\n",
       "      <td>5.044843</td>\n",
       "      <td>43.049327</td>\n",
       "      <td>13.228700</td>\n",
       "      <td>4.820628</td>\n",
       "      <td>1.345291</td>\n",
       "      <td>2.578475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.296451</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.966597</td>\n",
       "      <td>8.350731</td>\n",
       "      <td>0.730689</td>\n",
       "      <td>5.532359</td>\n",
       "      <td>68.893528</td>\n",
       "      <td>9.812109</td>\n",
       "      <td>0.313152</td>\n",
       "      <td>0.104384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>6.614786</td>\n",
       "      <td>1.945525</td>\n",
       "      <td>3.501946</td>\n",
       "      <td>4.863813</td>\n",
       "      <td>0.680934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291829</td>\n",
       "      <td>81.225681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.977413</td>\n",
       "      <td>0.205339</td>\n",
       "      <td>18.583162</td>\n",
       "      <td>33.880903</td>\n",
       "      <td>1.129363</td>\n",
       "      <td>14.989733</td>\n",
       "      <td>11.190965</td>\n",
       "      <td>3.285421</td>\n",
       "      <td>11.293634</td>\n",
       "      <td>2.464066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>38.255699</td>\n",
       "      <td>0.495540</td>\n",
       "      <td>4.063429</td>\n",
       "      <td>1.684836</td>\n",
       "      <td>2.675917</td>\n",
       "      <td>0.792864</td>\n",
       "      <td>0.891972</td>\n",
       "      <td>12.983152</td>\n",
       "      <td>0.594648</td>\n",
       "      <td>37.561943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5  \\\n",
       "0  43.265306   0.000000   1.122449   0.102041   0.306122   1.020408   \n",
       "1   0.000000  19.559471  41.850220  18.854626  15.594714   0.088106   \n",
       "2   2.034884   0.096899  59.496124  33.720930   0.775194   0.484496   \n",
       "3   0.396040   0.297030  10.693069  80.594059   0.297030   0.198020   \n",
       "4  27.800407   4.073320  11.405295   0.814664  27.393075   3.564155   \n",
       "5  10.089686   0.448430   7.399103  11.995516   5.044843  43.049327   \n",
       "6   2.296451   0.000000   3.966597   8.350731   0.730689   5.532359   \n",
       "7   6.614786   1.945525   3.501946   4.863813   0.680934   0.000000   \n",
       "8   2.977413   0.205339  18.583162  33.880903   1.129363  14.989733   \n",
       "9  38.255699   0.495540   4.063429   1.684836   2.675917   0.792864   \n",
       "\n",
       "           6          7          8          9  \n",
       "0  13.061224  41.122449   0.000000   0.000000  \n",
       "1   0.440529   3.524229   0.088106   0.000000  \n",
       "2   0.678295   2.519380   0.096899   0.096899  \n",
       "3   2.673267   4.158416   0.297030   0.396040  \n",
       "4   0.814664   4.276986   0.509165  19.348269  \n",
       "5  13.228700   4.820628   1.345291   2.578475  \n",
       "6  68.893528   9.812109   0.313152   0.104384  \n",
       "7   0.291829  81.225681   0.000000   0.875486  \n",
       "8  11.190965   3.285421  11.293634   2.464066  \n",
       "9   0.891972  12.983152   0.594648  37.561943  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_classes, Probs = nnet.use(Xtest)\n",
    "#print(Y_classes.shape)\n",
    "#print(\"into con\")\n",
    "confusion_matrix(Y_classes,Ttest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
